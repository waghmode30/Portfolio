# -*- coding: utf-8 -*-
"""FF_pass_using_math.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dLN22fU1ul6xukVj6udEutMPKb_h-HfA

Loading the training data through drive
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
import numpy as np
import pandas as pd

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train.shape, type(x_train) ,y_train.shape , type(y_train)

"""Splitting the data in two havles real and fake"""

x_train_split = x_train.shape[0] // 2
x_train_real = x_train[x_train_split:]
x_train_fake = x_train[:x_train_split]
print("X train real: ", x_train_real.shape)
print("X train fake: ", x_train_fake.shape)

y_train_real = y_train[x_train_split:]
y_train_fake = y_train[:x_train_split]
print("Y train real: ", y_train_real.shape)
print("Y train fake: ", y_train_fake.shape)

# Shuffling the y_train_fake labels to create fake data
np.random.shuffle(y_train_fake)

"""Encoding the real labels to real data"""

import numpy as np

output_size = 10

def one_hot_encode(y_train_real):
    encoded_labels = np.random.randint(50, 201, size=(y_train_real.shape[0], output_size))
    for i in range(y_train_real.shape[0]):
        encoded_labels[i, y_train_real[i]] = 255
        # print(encoded_labels[0])
    return encoded_labels


print((one_hot_encode(y_train_real).shape))

y_train_onehot_real = one_hot_encode(y_train_real)

# Loop through each sample in x_train_real
for i in range(x_train_real.shape[0]):
    # Assign the one-hot encoded label from y_train_onehot_real to the first 10 elements of the first row of x_train_real
    x_train_real[i, 0, :10] = y_train_onehot_real[i]

print(x_train_real[0])

import matplotlib.pyplot as plt

plt.imshow(x_train_real[3000], cmap='gray')
plt.xlabel("X")
plt.ylabel("Y")
plt.show()

"""Encoding the fake labels to fake data"""

output_size = 10

def one_hot_encode(y_train_fake):
    encoded_labels = np.random.randint(50, 201, size=(y_train_fake.shape[0], output_size))
    for i in range(y_train_fake.shape[0]):
        encoded_labels[i, y_train_fake[i]] = 255
        # print(encoded_labels[0])
    return encoded_labels


print((one_hot_encode(y_train_fake).shape))

y_train_onehot_fake = one_hot_encode(y_train_fake)

# Loop through each sample in x_train_real
for i in range(x_train_fake.shape[0]):
    # Assign the one-hot encoded label from y_train_onehot_fake to the first 10 elements of the first row of x_train_fake
    x_train_fake[i, 0, :10] = y_train_onehot_fake[i]

print(x_train_fake[0])

plt.imshow(x_train_fake[12313], cmap='gray')
plt.xlabel("X")
plt.ylabel("Y")
plt.show()

"""Normalizing the data"""

x_train_real_norm = x_train_real/255
x_train_fake_norm = x_train_fake/255
y_train_norm = y_train/255

x_train_real_norm.shape , x_train_fake_norm.shape

x_train_rf_norm = np.concatenate((x_train_real,x_train_fake), axis=0 )
x_train_rf_norm.shape

"""Defining the parameters and forward propogation"""

# Definin Hyperparameters
input_size = 784
hidden_size_one = 2000
hidden_size_two = 2000
output_size = 10
learning_rate = 0.01
num_epochs = 1000

# Initialzae the weights
np.random.seed(36)
weights_input_hidden_1 = np.random.randn(hidden_size_one, input_size)
bais_input_hidden_1 = np.random.randn(hidden_size_one,1)
weights_hidden_1_hidden_2 = np.random.randn(hidden_size_two,hidden_size_one)
bais_hidden_1_hidden_2 = np.random.randn(hidden_size_two,1)
weights_hidden_2_output = np.random.randn(output_size,hidden_size_two)
bais_hidden_2_output = np.random.randn(output_size,1)

# Sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

import numpy as np
from sklearn import preprocessing

# Forward pass and backpropagation
for epoch in range(num_epochs):
    epoch_error = 0
    num_correct = 0
    for sample in range(60000):
        # Forward pass
        x = x_train_rf_norm[:, sample].reshape(-1, 1)

        # Hidden layer 1
        hidden_layer_1_input = np.dot(weights_input_hidden_1, x) + bias_input_hidden_1
        hidden_layer_1_output = sigmoid(hidden_layer_1_input)

from sklearn import preprocessing

for epoch in range(num_epochs):
  epoch_error = []
  num_correct = 0
  for sample in range(60000):
    # Forward-Forward propogation

    # Input to hidden layer 1

# weights_input_hidden_1_tra = np.transpose(weights_input_hidden_1)
# weights_input_hidden_1_tra_repshape = np.reshape(weights_input_hidden_1_tra, (28,28,25))

    # forward pass
    x = x_train_rf_norm[:, sample].reshape(-1, 1)
    hidden_layer_1_input = np.dot(weights_input_hidden_1, x_train_rf_norm) + bais_input_hidden_1.T
    hidden_layer_1_input_y_sq = np.sum(hidden_layer_1_input**2)
    hidden_layer_1_input_thr = hidden_layer_1_input_y_sq - 3 #threshold
    hidden_layer_1_output = sigmoid(hidden_layer_1_input_thr)


    # loacl backporpogation with calculating error as MSE
    if hidden_layer_1_output < 0:
      hidden_layer_1_error = np.sum((x - hidden_layer_1_output)**2) / 60000
      weights_input_hidden_1 -= learning_rate * np.dot(hidden_layer_1_error, hidden_layer_1_output.T)
      bais_input_hidden_1  -= learning_rate * hidden_layer_1_error

    hidden_layer_1_output_norm = preprocessing.normalize([hidden_layer_1_output])


    # hidden layer 1 to hidden layer 2

    # forward pass
    hidden_layer_2_input = np.dot(weights_hidden_1_hidden_2, hidden_layer_1_output_norm) + bais_hidden_1_hidden_2
    hidden_layer_2_input_y_sq = np.sum(hidden_layer_2_input**2)
    hidden_layer_2_input_thr = hidden_layer_2_input_y_sq - 2 #threshold
    hidden_layer_2_output = sigmoid(hidden_layer_2_input_thr)

    # loacl backporpogation with calculating error as MSE
    if hidden_layer_2_output < 0:
      hidden_layer_2_error = np.sum((hidden_layer_1_output_norm - hidden_layer_2_output)**2) / 60000
      weights_hidden_1_hidden_2 -= learning_rate * np.dot(hidden_layer_2_error, hidden_layer_2_output.T)
      bais_hidden_1_hidden_2 -= learning_rate * hidden_layer_2_error

    hidden_layer_2_output_norm = preprocessing.normalize([hidden_layer_2_output])

    # Hidden layer 2 to output

    # forward pass
    output_layer_input = np.dot(weights_hidden_2_output,hidden_layer_2_output_norm) + bais_hidden_2_output
    output_layer_input_y_sq = np.sum(output_layer_input**2)
    output_layer_input_thr = output_layer_input_y_sq - 1 #threshold
    output_layer_output = sigmoid(output_layer_input_thr)

    # loacl backporpogation with calculating error as MSE
    if output_layer_output < 0:
      output_layer_error = np.sum((hidden_layer_2_output_norm - output_layer_output)**2) / 60000
      weights_hidden_2_output -= learning_rate * np.dot(output_layer_error, output_layer_output.T)
      bais_hidden_2_output -= learning_rate * output_layer_error

    output_layer_output_norm = preprocessing.normalize([output_layer_output])